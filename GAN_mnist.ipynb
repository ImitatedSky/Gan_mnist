{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNFcFPWaE6CCDJRB2LqSsH5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ImitatedSky/Gan_mnist/blob/main/GAN_mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtcpNAf6Gj-_"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.utils as vutils\n",
        "import PIL.Image as Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)\n",
        "           \n",
        "class CBR(nn.Sequential):\n",
        "    def __init__(self, in_planes, out_planes, kernel_size=3, stride=1, groups=1):\n",
        "        padding = (kernel_size - 1) // 2\n",
        "        norm_layer = nn.BatchNorm2d\n",
        "        super(CBR, self).__init__(\n",
        "            nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding, groups=groups, bias=False),\n",
        "            norm_layer(out_planes),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "class CBLR(nn.Sequential):\n",
        "    def __init__(self, in_planes, out_planes, kernel_size=3, stride=1, groups=1):\n",
        "        padding = (kernel_size - 1) // 2\n",
        "        norm_layer = nn.BatchNorm2d\n",
        "        super(CBLR, self).__init__(\n",
        "            nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding, groups=groups, bias=False),\n",
        "            norm_layer(out_planes),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "class TCBR(nn.Sequential):\n",
        "    def __init__(self, in_planes, out_planes, kernel_size=4, stride=2, padding=1):\n",
        "        padding = (kernel_size - 1) // 2\n",
        "        norm_layer = nn.BatchNorm2d\n",
        "        super(TCBR, self).__init__(\n",
        "            nn.ConvTranspose2d(in_planes, out_planes, kernel_size, stride, padding, bias=False),\n",
        "            norm_layer(out_planes),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )                    \n",
        "                                                 \n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, latents):\n",
        "        super(Generator, self).__init__()\n",
        "        \n",
        "        self.layer1= nn.Sequential(\n",
        "            # input is random_Z,  state size. latents x 1 x 1 \n",
        "            # going into a convolution\n",
        "            TCBR(latents, 256, 4, 2, 1),  # state size. 256 x 2 x 2\n",
        "            CBR(256, 128, 3, 1)\n",
        "        )\n",
        "        \n",
        "        self.layer2= nn.Sequential(\n",
        "            TCBR(128, 256, 4, 1, 0), # state size. 256 x 3 x 3\n",
        "            TCBR(256, 256, 4, 2, 1), # state size. 256 x 6 x 6\n",
        "            \n",
        "        )\n",
        "        self.layer3= nn.Sequential(\n",
        "            TCBR(256, 128, 4, 1, 0), # state size. 256 x 7 x 7\n",
        "            TCBR(128, 128, 4, 2, 1),  # state size. 256 x 14 x 14\n",
        "            CBR(128, 128, 3, 1)\n",
        "            # state size. 256 x 6 x 6\n",
        "\n",
        "        )\n",
        "        self.layer4= nn.Sequential(\n",
        "            TCBR(128, 64, 4, 2, 1), # state size. 64 x 28 x 28\n",
        "            CBR(64, 64, 3, 1),\n",
        "            CBR(64, 64, 3, 1),\n",
        "            nn.Conv2d(64, 1, 3, 1, 1), # state size. 1 x 28 x 28\n",
        "            nn.Tanh()\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        return x\n",
        "    \n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self,):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            CBLR(1, 32, 3, 2), # b*32*14*14\n",
        "            CBLR(32, 64, 3, 1), # b*64*14*14\n",
        "            CBLR(64, 128, 3, 2), # b*128*7*7\n",
        "            CBLR(128, 128, 3, 2), # b*32*3*3\n",
        "            CBLR(128, 64, 3, 2), # b*32*1*1\n",
        "        )        \n",
        "        self.fc = nn.Linear(64,2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = nn.functional.adaptive_avg_pool2d(x, 1).reshape(x.shape[0], -1)\n",
        "        ft = x\n",
        "        output = self.fc(x)\n",
        "        return output"
      ],
      "metadata": {
        "id": "5iFkByxDGvEv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}